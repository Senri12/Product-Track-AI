{
  "metadata": {
    "dialog_id": "dialog1327",
    "model_name": "granite3.2",
    "model_parameters": "2.0B",
    "system_prompt_id": "prompt15",
    "lecture_title": "1_Классификаторы_KNN_и_наивный_байес",
    "lecture_topic": "Как вычислить вероятность класса в Naive Bayes?",
    "f1": 30.6,
    "precision": 29.8,
    "recall": 47.4,
    "claim_recall": 35.5,
    "context_precision": 50.0,
    "context_utilization": 34.8,
    "hallucination": 13.5,
    "faithfulness": 31.2,
    "noise_sensitivity": 19.0,
    "self_knowledge": 5.4,
    "timestamp": "2025-10-28T13:24:20.146929"
  },
  "turns": [
    {
      "turn_number": 1,
      "role": "user",
      "content": "Как вычислить вероятность класса в Naive Bayes?",
      "model_response": "",
      "rating": ""
    },
    {
      "turn_number": 2,
      "role": "assistant",
      "content": "",
      "model_response": "В наивном байесовском классификаторе, когда объем тренировочных данных значительен и существует независимость между словами в тексте (аналогично по-настоящему на тему), вероятность класса класса y для каждого i-го признака X_i часто вычисляется как отношение количества слов X_i в тексте, отделянных на класс y, к общему количеству слов X_i в тексте. Это можно представить следующим образом:\n\n$$\\mathsf{P}(X_i | \\text{Класс} =y) = \\frac{\\text{количество слов } X_i \\text{ в класс y}}{\\text{общий количество слов } X_i \\text{ в тексте}}$$\n\nЗатем, для всего p предикторов (слов), вычисляется продукт независимых условных вероятностей:\n\n$$\\prod_{i=1}^{p} \\mathsf{P}\\left(X_i | \\text{Класс} =y\\right) = \\prod_{i=1}^{p} \\frac{\\text{количество слов } X_i \\text{ в класс y}}{\\text{общий количество слов } X_i \\text{ в тексте}}$$\n\nВ конце, вероятность каждого класса y вычисляется как:\n\n$$\\mathsf{P}(\\text{Класс} =y) = \\frac{\\text{количество слов в класс y в тексте}}{\\text{общий количество слов в тексте}}$$\n\nТаким образом, в наивном байесовском классификаторе вычисляются вероятности класса и условные вероятности, основанные на предположении независимости слов. Это позволяет выбрать класс, который привыкает большее количество слов в тексте, если текст относится к этому классу.\n\nЭтот подход делает наивном байесовским классификатором эффективным для текстового анализа, поскольку он вычисляет вероятности сложно в сравнении с другими методами классификации, но по-видимому достаточно хорошо справляется с текстовыми данными.",
      "rating": ""
    }
  ]
}