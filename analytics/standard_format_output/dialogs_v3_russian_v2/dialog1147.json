{
  "metadata": {
    "dialog_id": "dialog1147",
    "model_name": "granite3.2",
    "model_parameters": "2.0B",
    "system_prompt_id": "prompt9",
    "lecture_title": "1_Классификаторы_KNN_и_наивный_байес",
    "lecture_topic": "Как вычислить вероятность класса в Naive Bayes?",
    "f1": 31.0,
    "precision": 26.4,
    "recall": 52.5,
    "claim_recall": 35.5,
    "context_precision": 50.0,
    "context_utilization": 31.4,
    "hallucination": 11.5,
    "faithfulness": 34.0,
    "noise_sensitivity": 23.7,
    "self_knowledge": 4.5,
    "timestamp": "2025-10-28T13:24:20.160118"
  },
  "turns": [
    {
      "turn_number": 1,
      "role": "user",
      "content": "Как вычислить вероятность класса в Naive Bayes?",
      "model_response": "",
      "rating": ""
    },
    {
      "turn_number": 2,
      "role": "assistant",
      "content": "",
      "model_response": "В наивном байесовском классификаторе для частотных предикторов, как X₁, X₂, ..., X_(p), вероятности P(X_i | Класс = y) могут быть оценены как частоты тех или и тогох слов в данном классе при обучении. Это означает, что для каждого предиктора X_i (слова или токени), мы рассчитаем частоту нахождения этого предиктора в текстах, атрибутах объектов, классифицируемых как y.\n\nПредположим, у нас есть N классифицированных текстов (объекты) класса y и M категорий (слов). Считаем, что для каждого X_i:\n\n1. Имеется счетка количества наблюдаемых вхождений X_i в текстах/атрибуты класса y – суточной частотой X_i_y.\n2. Имеется счетка количества всех слов (предикторов) в текстах/атрибуты класса y – глобальная суточная частота.\n\nТогда вероятность P(X_i | Класс = y) может быть вычислена следующим образом:\n\nP(X_i | Класс = y) ≈ X_i_y / N\n\nЗдесь N — сумма суточных вхождений X_i в тексты/атрибуты класса y. Этот метод, когда объем тренировочных данных достаточно большой, даст более точные истины вероятностей по сравнению с методом, не оценивающих частоты в размерности.\n\nВ качестве примера, если у нас есть 1000 текстов класса y, содержащих слово \"Apple\", то количество заметок X_i = \"Apple\" в этих текстах будет 500. Сумма всех суточных вхождений для этого предиктора (X_i) в тексты/атрибуты класса y будет быть 15000 (предположим, что общая длина этих текстов составляет 15000 символов). Тогда вероятность P(X_i = \"Apple\" | Класс = y) будет примерно 500 / 15000 ≈ 0.033.\n\nПосле вычисления этой частот, мы можем затем использовать формулу классификатора наивного байеса:\n\ny^* = Arg max_(y ∈ Y) [P(Класс=y) * (∏_{i=1}^{p} P(X_i | Класс = y))]\n\nЭто позволяет нас найти класс среди Y, который имеет наибольшую вероятность в соответствии с предикторами X₁, X₂, ..., X_(p).",
      "rating": ""
    }
  ]
}