{
  "metadata": {
    "dialog_id": "dialog0481",
    "model_name": "mistral",
    "model_parameters": "7.0B",
    "system_prompt_id": "prompt2",
    "lecture_title": "1_Классификаторы_KNN_и_наивный_байес",
    "lecture_topic": "Что такое метод k-ближайших соседей?",
    "f1": 37.7,
    "precision": 36.0,
    "recall": 51.5,
    "claim_recall": 35.5,
    "context_precision": 50.0,
    "context_utilization": 32.5,
    "hallucination": 10.0,
    "faithfulness": 34.4,
    "noise_sensitivity": 21.1,
    "self_knowledge": 5.7,
    "timestamp": "2025-10-28T13:24:20.255834"
  },
  "turns": [
    {
      "turn_number": 1,
      "role": "user",
      "content": "Что такое метод k-ближайших соседей?",
      "model_response": "",
      "rating": ""
    },
    {
      "turn_number": 2,
      "role": "assistant",
      "content": "",
      "model_response": " Метод к-ближайших соседей (k-NN) — это алгоритм классификации и регрессии, основанный на том, что новое наблюдение будет принадлежать тому же классу или подвергаться такой же регрессионной модели, что является ближайшим среди k других наблюдений из обучающего набора. В частности, для двухклассовой задачи к-NN классифицирует новые данные как класс, в котором преобладают (более чем по 50%) ближайшие k объектов из обучающего набора.\n\nВ случае нечетного числа классов и к-NN с использованием весов каждому объекту приписывается некоторый вес, зависящий от тестового объекта, и результатом является тот класс, суммарный вес представителей которого среди k ближайших соседей наибольший. Этот подход известен как взвешенный к-NN.\n\nМетод к-ближайших соседей является непараметрическим, т.е. не требует определения а priori числа классов, пространственных характеристик и других параметров модели для классификации и регрессии данных. Однако он чрезвычайно затратен в вычислительном отношении, особенно на больших обучающих наборах, что приводит к необходимости оптимизации методов поиска ближайших соседей.",
      "rating": ""
    }
  ]
}