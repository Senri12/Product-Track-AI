{
  "data": [
    {
      "file_name": "1_Классификаторы_KNN_и_наивный_байес (2).tex",
      "qas": [
        {
          "query": "Что такое метод k-ближайших соседей?",
          "gt_answer": "Метод k-ближайших соседей (k-NN) — это алгоритм классификации, где для нового объекта класс определяется по большинству классов среди k ближайших объектов из тренировочного набора. Ближайшие определяются с помощью метрики расстояния. Чтобы уточнить понимание: как вы думаете, почему выбор k важен? Верный ответ: малое k может привести к переобучению из-за шума, большое k — к недообучению."
        },
        {
          "query": "Как выбрать количество соседей k в k-NN?",
          "gt_answer": "Выбор k осуществляется с помощью кросс-валидации или оценки ошибки на валидационном наборе. Малое k чувствительно к шуму, большое — сглаживает данные. Вопрос для отработки: что произойдет, если k равно размеру выборки? Верный ответ: все объекты будут учтены, и предсказание будет по большинству классов в всей выборке."
        },
        {
          "query": "Что такое евклидова метрика?",
          "gt_answer": "Евклидова метрика — это расстояние по прямой: d(x, x') = sqrt(sum (x_i - x'_i)^2). Уточняющий вопрос: в каком случае она подходит лучше манхэттенской? Верный ответ: для непрерывных данных без ограничений на движение."
        },
        {
          "query": "В чем суть наивного байесовского классификатора?",
          "gt_answer": "Наивный байесовский классификатор предполагает условную независимость признаков при заданном классе и использует формулу Байеса для вычисления вероятностей. Вопрос: почему он 'наивный'? Верный ответ: из-за сильного предположения о независимости, которое редко выполняется, но работает хорошо на практике."
        },
        {
          "query": "Как работает сглаживание по Лапласу?",
          "gt_answer": "Сглаживание по Лапласу добавляет 1 к каждому счетчику, чтобы избежать нулевых вероятностей: P(X|y) = (1 + count(X in y)) / (|V| + count(words in y)). Уточнение: зачем это нужно? Верный ответ: для обработки новых слов, не виданных в тренировке."
        },
        {
          "query": "Что такое манхэттенское расстояние?",
          "gt_answer": "Манхэттенское расстояние: d(x, x') = sum |x_i - x'_i|. Вопрос: когда его использовать? Верный ответ: в сеточных структурах, как в городах с кварталами."
        },
        {
          "query": "Как вычислить вероятность класса в Naive Bayes?",
          "gt_answer": "P(class | features) = [P(features | class) * P(class)] / P(features), с наивным предположением. Отработка: если признаки независимы, как упрощается? Верный ответ: P(features | class) = prod P(feature_i | class)."
        },
        {
          "query": "Что такое расстояние Чебышёва?",
          "gt_answer": "d(x, x') = max |x_i - x'_i|. Уточнение: для чего полезно? Верный ответ: для максимального отклонения по координатам."
        },
        {
          "query": "Как классифицировать письмо как спам в Naive Bayes?",
          "gt_answer": "Вычислить P(spam | words) и P(not spam | words), выбрать максимум. Вопрос: что если слово не в словаре? Верный ответ: использовать Laplace smoothing или игнорировать."
        },
        {
          "query": "В чем разница между k-NN и взвешенным k-NN?",
          "gt_answer": "В взвешенном k-NN голоса соседей взвешиваются по расстоянию. Отработка: почему это лучше? Верный ответ: ближние соседи имеют больше влияния."
        },
        {
          "query": "Вычисли вероятность спама для слова 'приз' из примера.",
          "gt_answer": "Из примера: P(спам | приз) = (5/32 * 32/80) / (8/80) = 5/8 = 0.625. Объяснение: используем формулу Байеса. Аналогичное задание: вычисли для слова 'спам' с P(спам|спам)=32/32, P(спам)=32/80."
        },
        {
          "query": "Вычисли расстояние Евклида между (1,2) и (3,4).",
          "gt_answer": "sqrt((3-1)^2 + (4-2)^2) = sqrt(4+4)=sqrt(8)=2*sqrt(2). Объяснение: формула корень из суммы квадратов. Аналог: между (0,0) и (1,1)."
        },
        {
          "query": "В примере с погодой, вычисли F(да) для теста.",
          "gt_answer": "ln(5/9) + ln(3/5) + ln(1/5) + ln(3/5) + ln(2/5) ≈ -4.135. Объяснение: суммируем лог-вероятности. Аналог: вычисли для 'нет'."
        },
        {
          "query": "Вычисли P(да | пасмурно, холодно, влажно, да).",
          "gt_answer": "≈0.606. Объяснение: 1 / (1 + exp(F(нет)-F(да))). Аналог: если изменить влажность на сухо, пересчитай."
        },
        {
          "query": "В спам-примере, вычисли P(win|спам) с Laplace.",
          "gt_answer": "(1+1)/(8+7)=2/15. Объяснение: добавляем 1 к счетчикам. Аналог: P(again|не спам)."
        }
      ]
    },
    {
      "file_name": "3. Инструменты обучения НС (2).tex",
      "qas": [
        {
          "query": "Что такое autograd?",
          "gt_answer": "Autograd — это движок автоматического дифференцирования для вычисления градиентов в графе вычислений. Уточнение: зачем нужен? Верный ответ: для автоматизации backpropagation без ручного кода градиентов."
        },
        {
          "query": "Что такое TensorFlow?",
          "gt_answer": "TensorFlow — библиотека для машинного обучения, фокусирующаяся на нейронных сетях, с поддержкой GPU и autograd. Вопрос: в чем отличие от PyTorch? Верный ответ: TensorFlow использует статические графы по умолчанию, PyTorch — динамические."
        },
        {
          "query": "Что такое Keras?",
          "gt_answer": "Keras — высокоуровневый API над TensorFlow для упрощения построения моделей. Отработка: когда использовать? Верный ответ: для простых задач, где гибкость не критична."
        },
        {
          "query": "Что такое over-fitting?",
          "gt_answer": "Over-fitting — когда модель слишком хорошо подстраивается под тренировку, но плохо на новых данных. Уточнение: как избежать? Верный ответ: регуляризация, early stopping."
        },
        {
          "query": "Что такое SGD?",
          "gt_answer": "Stochastic Gradient Descent — градиентный спуск на батчах. Вопрос: почему стохастический? Верный ответ: использует подвыборки, а не весь датасет."
        },
        {
          "query": "Что такое Momentum в оптимизации?",
          "gt_answer": "Momentum добавляет инерцию: v_t = γ v_{t-1} - η ∇L. Уточнение: зачем? Верный ответ: ускоряет в нужном направлении, снижает колебания."
        },
        {
          "query": "Что такое Adam?",
          "gt_answer": "Adam — адаптивный оптимизатор, комбинирующий Momentum и RMSProp. Отработка: почему популярен? Верный ответ: адаптивный learning rate по параметрам."
        },
        {
          "query": "Что такое learning rate?",
          "gt_answer": "Learning rate — шаг спуска в градиентном методе. Вопрос: как выбрать? Верный ответ: начинать большим, уменьшать gradually."
        },
        {
          "query": "Что такое метод Ньютона?",
          "gt_answer": "Метод второго порядка, использующий гессиан для аппроксимации. Уточнение: минусы? Верный ответ: вычислительно дорогой для больших сетей."
        },
        {
          "query": "Что такое RMSProp?",
          "gt_answer": "RMSProp адаптирует learning rate по скользящему среднему квадратов градиентов. Вопрос: отличие от Adagrad? Верный ответ: не накапливает все градиенты, а экспоненциально затухает."
        },
        {
          "query": "Вычисли шаг в SGD для L(w)=w^2, w=3, η=0.1.",
          "gt_answer": "∇L=2w=6, w_new=3-0.1*6=2.4. Объяснение: спуск по градиенту. Аналог: для L(w)=(w-1)^2, w=0, η=0.5."
        },
        {
          "query": "Вычисли v_t в Momentum: γ=0.9, v_{t-1}=1, ∇L=2, η=0.1.",
          "gt_answer": "v_t=0.9*1 - 0.1*2=0.9-0.2=0.7. Объяснение: инерция + градиент. Аналог: γ=0.5, v_{t-1}=0, ∇L=3, η=0.01."
        },
        {
          "query": "В Adagrad, G_{t-1,ii}=4, g_t=2, η=0.1, ε=1e-8.",
          "gt_answer": "Δθ = -0.1 / sqrt(4 + 4 + 1e-8) * 2 ≈ -0.1 / sqrt(8) * 2 ≈ -0.1*0.707*2 ≈ -0.141. Объяснение: адаптивный шаг. Аналог: G=9, g=1."
        },
        {
          "query": "Вычисли learning rate decay: η0=0.1, t=5, T=10, linear.",
          "gt_answer": "η=0.1*(1-5/10)=0.05. Объяснение: линейное затухание. Аналог: экспоненциальное, η0=0.1, t=2, T=3."
        },
        {
          "query": "В Nesterov, вычисли ∇ в ω - γ v_{t-1}: γ=0.9, v=1, ∇=2.",
          "gt_answer": "Сначала ω_temp = ω - 0.9*1, затем ∇ там. (Упрощенно). Объяснение: lookahead. Аналог: простой случай."
        }
      ]
    }
  ]
}