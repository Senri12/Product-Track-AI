#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¢–†–ï–• –≤–µ—Ä—Å–∏–π –¥–∞–Ω–Ω—ã—Ö RAGChecker
Version 1: –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã
Version 2: —Ä—É—Å—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã (–ø–µ—Ä–≤–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è)
Version 3: —Ä—É—Å—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã (–≤—Ç–æ—Ä–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è)

–° –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –æ–ø–∏—Å–∞–Ω–∏–π –º–µ—Ç—Ä–∏–∫ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º —Å –±–µ–Ω—á–º–∞—Ä–∫–æ–º –∏–∑ —Å—Ç–∞—Ç—å–∏
"""

import pandas as pd
import json
import os
from pathlib import Path
from openpyxl import load_workbook
from openpyxl.styles import PatternFill, Font, Alignment, Border, Side
from openpyxl.utils import get_column_letter
from openpyxl.formatting.rule import ColorScaleRule, CellIsRule, DataBarRule
from openpyxl.comments import Comment
import numpy as np

# –¶–≤–µ—Ç–∞
GREEN_FILL = PatternFill(start_color='C6EFCE', end_color='C6EFCE', fill_type='solid')
YELLOW_FILL = PatternFill(start_color='FFEB9C', end_color='FFEB9C', fill_type='solid')
RED_FILL = PatternFill(start_color='FFC7CE', end_color='FFC7CE', fill_type='solid')
LIGHT_BLUE_FILL = PatternFill(start_color='DDEBF7', end_color='DDEBF7', fill_type='solid')
LIGHT_GREEN_FILL = PatternFill(start_color='E2EFDA', end_color='E2EFDA', fill_type='solid')
HEADER_FILL = PatternFill(start_color='4472C4', end_color='4472C4', fill_type='solid')
HEADER_FONT = Font(bold=True, color='FFFFFF')

# –û–ø–∏—Å–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫
METRICS_DESCRIPTIONS = {
    'precision': '–¢–æ—á–Ω–æ—Å—Ç—å: –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –≤ –æ—Ç–≤–µ—Ç–µ –º–æ–¥–µ–ª–∏. –í—ã—Å–æ–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ = –º–∞–ª–æ –ª–∏—à–Ω–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏',
    'recall': '–ü–æ–ª–Ω–æ—Ç–∞: –¥–æ–ª—è —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π, –ø–æ–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª—å—é. –í—ã—Å–æ–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ = –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç',
    'f1': 'F1-–º–µ—Ä–∞: –±–∞–ª–∞–Ω—Å —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–Ω–æ—Ç—ã. –ì–õ–ê–í–ù–ê–Ø –ú–ï–¢–†–ò–ö–ê –ö–ê–ß–ï–°–¢–í–ê. >60% - –æ—Ç–ª–∏—á–Ω–æ, 40-60% - —Ö–æ—Ä–æ—à–æ',
    'claim_recall': 'Claim Recall: –¥–æ–ª—è —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –≤ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–∞—Ö. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ retriever',
    'context_precision': 'Context Precision: –¥–æ–ª—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å—Ä–µ–¥–∏ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö. –í—ã—Å–æ–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ = –º–∞–ª–æ —à—É–º–∞',
    'context_utilization': 'Context Utilization: –Ω–∞—Å–∫–æ–ª—å–∫–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç',
    'noise_sensitivity_relevant': 'Relevant Noise Sensitivity: —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ —à—É–º—É –≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –ù–∏–∑–∫–æ–µ = —Ö–æ—Ä–æ—à–æ',
    'noise_sensitivity_irrelevant': 'Irrelevant Noise Sensitivity: —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É. –ù–∏–∑–∫–æ–µ = —Ö–æ—Ä–æ—à–æ',
    'hallucination': 'Hallucination: –¥–æ–ª—è "–∏–∑–æ–±—Ä–µ—Ç–µ–Ω–Ω—ã—Ö" —Ñ–∞–∫—Ç–æ–≤. –ö–†–ò–¢–ò–ß–ù–û! <10% - –æ—Ç–ª–∏—á–Ω–æ, >50% - –Ω–µ–ø—Ä–∏–µ–º–ª–µ–º–æ',
    'self_knowledge': 'Self-knowledge: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –º–æ–¥–µ–ª–∏. –ú–æ–∂–µ—Ç –±—ã—Ç—å + –∏–ª–∏ - –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏',
    'faithfulness': 'Faithfulness: —Ç–æ—á–Ω–æ—Å—Ç—å —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É. >80% - –æ—Ç–ª–∏—á–Ω–æ. –°–≤—è–∑–∞–Ω–æ —Å hallucination'
}

# –ë–µ–Ω—á–º–∞—Ä–∫ –∏–∑ —Å—Ç–∞—Ç—å–∏ (ClapNQ, –ª—É—á—à–∏–µ —Å–∏—Å—Ç–µ–º—ã)
BENCHMARK_DATA = {
    'E5-Mistral_GPT-4': {
        'precision': 59.7, 'recall': 51.1, 'f1': 47.9,
        'claim_recall': 81.5, 'context_precision': 43.6,
        'context_utilization': 59.9,
        'noise_sensitivity_relevant': 31.1,
        'noise_sensitivity_irrelevant': 3.8,
        'hallucination': 5.4,
        'self_knowledge': 2.3,
        'faithfulness': 92.3
    },
    'BM25_GPT-4': {
        'precision': 56.9, 'recall': 50.0, 'f1': 46.7,
        'claim_recall': 81.1, 'context_precision': 41.3,
        'context_utilization': 56.4,
        'noise_sensitivity_relevant': 29.4,
        'noise_sensitivity_irrelevant': 5.9,
        'hallucination': 7.5,
        'self_knowledge': 2.2,
        'faithfulness': 90.3
    }
}

def load_prompts(version_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ JSON —Ñ–∞–π–ª–∞"""
    prompts_file = Path(version_path) / "system_prompts.json"

    if not prompts_file.exists():
        print(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {prompts_file}")
        return {}

    with open(prompts_file, 'r', encoding='utf-8') as f:
        prompts_data = json.load(f)

    prompts_dict = {}
    for prompt in prompts_data:
        prompt_id = prompt['system_prompt_id']
        prompts_dict[prompt_id] = {
            'prompt': prompt['system_prompt'],
            'description': prompt.get('description', ''),
            'version': prompt.get('version', ''),
        }

    return prompts_dict

def load_ragchecker_metrics_for_version(version_path, version_name):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç—Ä–∏–∫ RAGChecker –¥–ª—è –æ–¥–Ω–æ–π –≤–µ—Ä—Å–∏–∏"""
    metrics_data = []
    ragchecker_path = Path(version_path) / "RAGChecker_outputs"

    if not ragchecker_path.exists():
        print(f"–ü—É—Ç—å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {ragchecker_path}")
        return pd.DataFrame()

    for model_dir in ragchecker_path.iterdir():
        if not model_dir.is_dir():
            continue

        model_name = model_dir.name

        for prompt_dir in model_dir.iterdir():
            if not prompt_dir.is_dir():
                continue

            prompt_id = prompt_dir.name
            json_files = list(prompt_dir.glob("ragchecker_report_*.json"))

            if not json_files:
                continue

            json_file = json_files[0]

            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                overall = data.get('overall_metrics', {})
                retriever = data.get('retriever_metrics', {})
                generator = data.get('generator_metrics', {})

                metrics_data.append({
                    'version': version_name,
                    'model_name': model_name,
                    'prompt_id': prompt_id,
                    'precision': overall.get('precision', None),
                    'recall': overall.get('recall', None),
                    'f1': overall.get('f1', None),
                    'claim_recall': retriever.get('claim_recall', None),
                    'context_precision': retriever.get('context_precision', None),
                    'context_utilization': generator.get('context_utilization', None),
                    'noise_sensitivity_relevant': generator.get('noise_sensitivity_in_relevant', None),
                    'noise_sensitivity_irrelevant': generator.get('noise_sensitivity_in_irrelevant', None),
                    'hallucination': generator.get('hallucination', None),
                    'self_knowledge': generator.get('self_knowledge', None),
                    'faithfulness': generator.get('faithfulness', None),
                })

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {json_file}: {e}")

    return pd.DataFrame(metrics_data)

def create_comparison_excel(version_paths, output_file):
    """–°–æ–∑–¥–∞–Ω–∏–µ Excel –æ—Ç—á–µ—Ç–∞ —Å–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º –≤—Å–µ—Ö –≤–µ—Ä—Å–∏–π"""

    print("="*70)
    print("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¢–†–ï–• –≤–µ—Ä—Å–∏–π RAGChecker")
    print("="*70)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö –≤–µ—Ä—Å–∏–π
    all_versions_data = {}
    all_prompts = {}

    for ver_name, ver_path in version_paths.items():
        print(f"\n{ver_name}: –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤...")
        prompts = load_prompts(ver_path)
        all_prompts[ver_name] = prompts
        print(f"  {len(prompts)} –ø—Ä–æ–º–ø—Ç–æ–≤")

        print(f"{ver_name}: –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç—Ä–∏–∫...")
        df = load_ragchecker_metrics_for_version(ver_path, ver_name)
        all_versions_data[ver_name] = df
        print(f"  {len(df)} –∑–∞–ø–∏—Å–µ–π")

    # –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
    print("\n–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü...")

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –≤–µ—Ä—Å–∏–∏
    df_all = pd.concat(all_versions_data.values(), ignore_index=True)

    # –°–æ–∑–¥–∞–µ–º Excel
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:

        # 1. –û–ø–∏—Å–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        metrics_desc_df = pd.DataFrame([
            {'–ú–µ—Ç—Ä–∏–∫–∞': k, '–û–ø–∏—Å–∞–Ω–∏–µ': v, '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': get_metric_category(k)}
            for k, v in METRICS_DESCRIPTIONS.items()
        ])
        metrics_desc_df.to_excel(writer, sheet_name='üìñ –û–ø–∏—Å–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫', index=False)

        # 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –±–µ–Ω—á–º–∞—Ä–∫–æ–º
        benchmark_df = pd.DataFrame(BENCHMARK_DATA).T
        benchmark_df['source'] = 'RAGChecker Paper (ClapNQ)'

        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—à–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        our_results = {}
        for ver_name, df in all_versions_data.items():
            our_results[ver_name] = df.mean(numeric_only=True).to_dict()
            our_results[ver_name]['source'] = f'Our results ({ver_name})'

        our_results_df = pd.DataFrame(our_results).T
        comparison_with_benchmark = pd.concat([benchmark_df, our_results_df])
        comparison_with_benchmark.to_excel(writer, sheet_name='üìä vs –ë–µ–Ω—á–º–∞—Ä–∫')

        # 3. –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤–µ—Ä—Å–∏—è–º
        version_summary = df_all.groupby('version').agg({
            'precision': ['mean', 'std'],
            'recall': ['mean', 'std'],
            'f1': ['mean', 'std'],
            'hallucination': ['mean', 'std'],
            'faithfulness': ['mean', 'std']
        }).round(2)
        version_summary.to_excel(writer, sheet_name='üìà –°–≤–æ–¥–∫–∞ –ø–æ –≤–µ—Ä—Å–∏—è–º')

        # 4. –î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –≤–µ—Ä—Å–∏–π
        df_all.to_excel(writer, sheet_name='üìã –í—Å–µ –¥–∞–Ω–Ω—ã–µ', index=False)

        # 5. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ V1 vs V2
        df_v1 = all_versions_data.get('v1_english', pd.DataFrame())
        df_v2 = all_versions_data.get('v2_russian', pd.DataFrame())
        if not df_v1.empty and not df_v2.empty:
            comparison_v1_v2 = compare_two_versions(df_v1, df_v2, 'v1', 'v2')
            comparison_v1_v2.to_excel(writer, sheet_name='V1 vs V2', index=False)

        # 6. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ V2 vs V3
        df_v3 = all_versions_data.get('v3_russian_v2', pd.DataFrame())
        if not df_v2.empty and not df_v3.empty:
            comparison_v2_v3 = compare_two_versions(df_v2, df_v3, 'v2', 'v3')
            comparison_v2_v3.to_excel(writer, sheet_name='V2 vs V3', index=False)

        # 7. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ V1 vs V3
        if not df_v1.empty and not df_v3.empty:
            comparison_v1_v3 = compare_two_versions(df_v1, df_v3, 'v1', 'v3')
            comparison_v1_v3.to_excel(writer, sheet_name='V1 vs V3', index=False)

        # 8. –ú–∞—Ç—Ä–∏—Ü–∞ F1 –ø–æ –≤–µ—Ä—Å–∏—è–º
        pivot_f1 = df_all.pivot_table(
            values='f1',
            index=['model_name', 'prompt_id'],
            columns='version',
            aggfunc='mean'
        ).round(2)
        pivot_f1.to_excel(writer, sheet_name='F1 –ø–æ –≤–µ—Ä—Å–∏—è–º')

        # 9. –õ—É—á—à–∏–µ —É–ª—É—á—à–µ–Ω–∏—è
        if 'v1_english' in all_versions_data and 'v3_russian_v2' in all_versions_data:
            improvements = find_improvements(df_v1, df_v3)
            improvements.to_excel(writer, sheet_name='üü¢ –£–ª—É—á—à–µ–Ω–∏—è V1‚ÜíV3', index=False)

        # 10. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤
        prompts_comparison = create_prompts_comparison(all_prompts)
        prompts_comparison.to_excel(writer, sheet_name='üìù –ü—Ä–æ–º–ø—Ç—ã', index=False)

    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    print("\n–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è...")
    wb = load_workbook(output_file)

    format_metrics_description_sheet(wb['üìñ –û–ø–∏—Å–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫'])
    format_benchmark_sheet(wb['üìä vs –ë–µ–Ω—á–º–∞—Ä–∫'])
    format_version_summary_sheet(wb['üìà –°–≤–æ–¥–∫–∞ –ø–æ –≤–µ—Ä—Å–∏—è–º'])

    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
    add_metric_comments(wb['üìã –í—Å–µ –¥–∞–Ω–Ω—ã–µ'])

    wb.save(output_file)

    print(f"\n‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
    print_summary_stats(all_versions_data)

def get_metric_category(metric_name):
    """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –º–µ—Ç—Ä–∏–∫–∏"""
    if metric_name in ['precision', 'recall', 'f1']:
        return 'Overall'
    elif metric_name in ['claim_recall', 'context_precision']:
        return 'Retriever'
    else:
        return 'Generator'

def compare_two_versions(df1, df2, name1, name2):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö –≤–µ—Ä—Å–∏–π"""
    merged = pd.merge(
        df1, df2,
        on=['model_name', 'prompt_id'],
        suffixes=(f'_{name1}', f'_{name2}'),
        how='outer'
    )

    metrics = ['precision', 'recall', 'f1', 'hallucination', 'faithfulness']
    for metric in metrics:
        col1 = f'{metric}_{name1}'
        col2 = f'{metric}_{name2}'
        if col1 in merged.columns and col2 in merged.columns:
            merged[f'{metric}_diff'] = merged[col2] - merged[col1]
            merged[f'{metric}_pct'] = ((merged[col2] - merged[col1]) / merged[col1].replace(0, np.nan) * 100).round(2)

    return merged

def find_improvements(df1, df3):
    """–ù–∞–π—Ç–∏ —É–ª—É—á—à–µ–Ω–∏—è –º–µ–∂–¥—É V1 –∏ V3"""
    comparison = compare_two_versions(df1, df3, 'v1', 'v3')
    improvements = comparison[comparison['f1_diff'] > 0].sort_values('f1_diff', ascending=False)
    return improvements[['model_name', 'prompt_id', 'f1_v1', 'f1_v3', 'f1_diff', 'f1_pct']]

def create_prompts_comparison(all_prompts):
    """–°–æ–∑–¥–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤"""
    rows = []
    prompt_ids = set()
    for prompts in all_prompts.values():
        prompt_ids.update(prompts.keys())

    for prompt_id in sorted(prompt_ids):
        row = {'prompt_id': prompt_id}
        for ver_name, prompts in all_prompts.items():
            if prompt_id in prompts:
                row[f'{ver_name}_prompt'] = prompts[prompt_id]['prompt']
                row[f'{ver_name}_desc'] = prompts[prompt_id]['description']
        rows.append(row)

    return pd.DataFrame(rows)

def format_metrics_description_sheet(ws):
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–∏—Å—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫"""
    for cell in ws[1]:
        cell.fill = HEADER_FILL
        cell.font = HEADER_FONT
        cell.alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)

    # –ê–≤—Ç–æ-—à–∏—Ä–∏–Ω–∞
    ws.column_dimensions['A'].width = 30
    ws.column_dimensions['B'].width = 100
    ws.column_dimensions['C'].width = 15

    ws.freeze_panes = 'A2'

def format_benchmark_sheet(ws):
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–∏—Å—Ç–∞ —Å –±–µ–Ω—á–º–∞—Ä–∫–æ–º"""
    for cell in ws[1]:
        cell.fill = HEADER_FILL
        cell.font = HEADER_FONT
        cell.alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)

    # –ü–æ–¥—Å–≤–µ—Ç–∫–∞ —Å—Ç—Ä–æ–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞
    for row in range(2, ws.max_row + 1):
        source_cell = ws.cell(row, ws.max_column)
        if 'Paper' in str(source_cell.value):
            for col in range(1, ws.max_column + 1):
                ws.cell(row, col).fill = LIGHT_BLUE_FILL
                ws.cell(row, col).font = Font(bold=True)

    ws.freeze_panes = 'B2'

def format_version_summary_sheet(ws):
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –ø–æ –≤–µ—Ä—Å–∏—è–º"""
    for cell in ws[1]:
        cell.fill = HEADER_FILL
        cell.font = HEADER_FONT
        cell.alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)

    ws.freeze_panes = 'B2'

def add_metric_comments(ws):
    """–î–æ–±–∞–≤–∏—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º –º–µ—Ç—Ä–∏–∫"""
    header_row = [cell.value for cell in ws[1]]

    for idx, col_name in enumerate(header_row, 1):
        if col_name in METRICS_DESCRIPTIONS:
            cell = ws.cell(1, idx)
            comment = Comment(METRICS_DESCRIPTIONS[col_name], 'RAGChecker')
            cell.comment = comment
            cell.fill = YELLOW_FILL  # –ñ–µ–ª—Ç–∞—è –∑–∞–ª–∏–≤–∫–∞ –¥–ª—è –∫–æ–ª–æ–Ω–æ–∫ —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏

def print_summary_stats(all_versions_data):
    """–í—ã–≤–µ—Å—Ç–∏ —Å–≤–æ–¥–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
    print("\n" + "="*70)
    print("–°–í–û–î–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –í–ï–†–°–ò–Ø–ú")
    print("="*70)

    for ver_name, df in all_versions_data.items():
        if df.empty:
            continue
        print(f"\n{ver_name}:")
        print(f"  F1:           {df['f1'].mean():.2f}%")
        print(f"  Precision:    {df['precision'].mean():.2f}%")
        print(f"  Recall:       {df['recall'].mean():.2f}%")
        print(f"  Hallucination: {df['hallucination'].mean():.2f}%")
        print(f"  Faithfulness: {df['faithfulness'].mean():.2f}%")

    print("\n" + "="*70)

if __name__ == '__main__':
    version_paths = {
        'v1_english': ' version_1',  # –ü—Ä–æ–±–µ–ª –≤ –Ω–∞—á–∞–ª–µ!
        'v2_russian': 'version_2',
        'v3_russian_v2': 'version_3'
    }

    output_file = 'comparison_all_versions.xlsx'

    create_comparison_excel(version_paths, output_file)
